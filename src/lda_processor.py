import fitz  # PyMuPDF
import unicodedata
import string
import spacy
import gensim
from gensim import corpora
import configparser
from nltk.corpus import stopwords
import os


class LicitacionTextProcessor:
    def __init__(self, df, config_file="./config/scraper_config.ini"):
        self.df = df.copy()
        self.config = configparser.ConfigParser()
        self.config.read(config_file)

        self.input_dir_pdf = self.config.get('input_output_path', 'output_dir_pdf', fallback="./pdfs")

        self.palabras_tecnologia = self._get_keywords('palabras_clave_tecnologia')
        self.palabras_descartes = self._get_keywords('palabras_descarte_tecnologia')
        
        #  Cargar modelo de spaCy en espa√±ol
        self.nlp = spacy.load("es_core_news_sm")
        self.nlp.max_length = 2000000  

        self.stop_custom = {'mucha', 'casos', 'alli','actuales', 'mio', 'poca', 'respectiva', 'ninguna', 'pocas', 
                            'actual','tambien', 'tipo', 'misma', 'cierto', 'veces', 'dentro', 'cierta', 'menor', 'ejemplo',
                            'partes','generales', 'forma', 'cuyas', 'muchas', 'realizadas', 'posible', 'respectivos', 'nueva', 
                             'ciertos', 'modo', 'segundo', 'ser', 'realizado', 'primera', 'realizada', 'respectivo', 'formas', 
                             'primeras', 'propias', 'nuevos', 'vez', 'tipos', 'dicho', 'base', 'mediante', 'posibles', 
                             'propio', 'respectivas', 'realizar', 'bajo', 'realizados', 'realiza', 'aqui', 'acuerdo', 'pocos',
                               'nuevo', 'anterior', 'posteriores', 'primero', 'general', 'alguno', 'cuya', 
                               'mismas', 'puede', 'despues', 'ejemplos', 'mismo', 'nuevas', 'segun', 'asi', 'ninguno', 
                               'ciertas', 'detras', 'cuales', 'segundos', 'ahi', 'propia', 'cuyo', 'segunda', 'primeros', 
                               'caso', 'realizacion', 'modos', 'conforme', 'hacia', 'cada', 'usted', 'mayor', 'propios', 
                               'posterior', 'respecto', 'segundas', 'anteriores', 'etc', 'parte', 'cuyos', 'ustedes'}
        self.stop_custom_completed = set(stopwords.words('spanish')) | self.stop_custom

        self.textos_limpios = []

    def _get_keywords(self, section):
        if section not in self.config:
            return []
        return list(self.config.options(section))

   # Funci√≥n para leer PDF
    def _extraer_texto_pdf(self, ruta):
        print(f"üìÑ Extrayendo texto de: {ruta}")
        try:
            doc = fitz.open(ruta)
            texto = ""
            for pagina in doc:
                texto += pagina.get_text()
            return texto
        except Exception as e:
            print(f"‚ö†Ô∏è Error leyendo {ruta}: {e}")
            return ""
        
    #  Limpiar y lematizar texto
    def _limpiar_y_tokenizar(self, texto):
        print("üßπ Limpiando y tokenizando texto...")

        # Normalizaci√≥n y limpieza previa
        texto = unicodedata.normalize("NFD", texto).encode("ascii", "ignore").decode("utf-8").lower()
        texto = texto.translate(str.maketrans('', '', string.punctuation))
        
        max_chars = self.nlp.max_length  # Usa el l√≠mite actual definido en spaCy
        tokens = []

        for i in range(0, len(texto), max_chars):
            chunk = texto[i:i + max_chars]
            doc = self.nlp(chunk)
            tokens.extend([
                token.lemma_ for token in doc
                if token.is_alpha and not token.is_stop and len(token.lemma_) > 2
            ])
        return tokens
    
    def _modelo_lda(self,corpus,diccionario, num_temas = 5):
        print("‚ö° Aplicando modelo LDA...")
        lda_model = gensim.models.LdaModel(
            corpus=corpus,
            id2word=diccionario,
            num_topics = num_temas,           
            random_state=42,
            passes=10, # cuantas veces se pasa por el corpus
            alpha='auto'
        )
        temas = lda_model.get_document_topics(corpus[0])
        return lda_model, sorted(temas, key=lambda x: -x[1])
        

    def _procesar_textos(self):
        print("üöÄ Procesando textos de los PDFs...")
        textos = []
        resultados_lda = []
        textos_limpios = []
        for _, row in self.df.iterrows():
            nombre_pdf = str(row.get('pdf', '')).strip()
            if not nombre_pdf or nombre_pdf.lower() == 'nan':
                textos.append("")
                textos_limpios.append([])
                resultados_lda.append("Sin tema") 
                continue
            ruta = os.path.join(self.input_dir_pdf, nombre_pdf)
            # 1 - Extracci√≥n de texto
            texto = self._extraer_texto_pdf(ruta)
            # 2 - Limpieza y tokenizaci√≥n
            tokens = self._limpiar_y_tokenizar(texto)
            textos_limpios.append(tokens)
            # 3 - Entrenar modelo LDA
            # 3.1 - Preparaci√≥n del corpus para modelo LDA (Se trata el documento como una "lista de palabras")
            texts = [tokens]
            print(f"üì¶ N¬∞ de documentos tokenizados: {len(texts)}")
        
            # 3.2 - Crear diccionario y corpus
            diccionario = corpora.Dictionary(texts)
            corpus = [diccionario.doc2bow(texto) for texto in texts]

            # 4 - Validaci√≥n antes de aplicar modelo
            if not corpus or all(len(doc) == 0 for doc in corpus) or len(diccionario) == 0:
                print("‚ö†Ô∏è Corpus o diccionario vac√≠o. Se asigna 'Sin tema'.")
                resultados_lda.append("Sin tema")
                continue

            # 5 - Aplicaci√≥n del modelo LDA
            lda_model, temas = self._modelo_lda(corpus=corpus, diccionario=diccionario)

            # 6 - Descripci√≥n de temas
            descripciones = []
            for id_tema, prob in temas:
                prob = round(prob, 2)
                if prob <= 0.0:
                    continue
                palabras = ", ".join([p for p, _ in lda_model.show_topic(id_tema, topn=10)])
                descripciones.append(f"{palabras} ({prob})")

            resultados_lda.append(" | ".join(descripciones) if descripciones else "Sin tema")
        if len(resultados_lda) != len(self.df):
             raise ValueError(f"‚ùå Longitud de resultados_lda ({len(resultados_lda)}) no coincide con el DataFrame ({len(self.df)}).")
        self.df["topicos_lda"] = resultados_lda
        self.textos_limpios = textos_limpios
        print("‚úÖ LDA completado y a√±adido al DataFrame.")
        return self.df
            
    def aplicar_clasificacion_manual(self, fallback_columna="descripcion"):
        print("‚ö° Aplicando clasificaci√≥n tecnologica/no teconol√≥gica (sobre texto de PDF)...")

        es_tecnologica = []
        es_no_tecnologica = []

        for idx, row in self.df.iterrows():
            # Usa texto limpio si est√° disponible, sino fallback
            if idx < len(self.textos_limpios) and self.textos_limpios[idx]:
                texto = " ".join(self.textos_limpios[idx])  # tokens a string
            else:
                texto = str(row.get(fallback_columna, "")).lower()

            contiene_tec = any(p in texto for p in self.palabras_tecnologia)
            contiene_no_tec = any(p in texto for p in self.palabras_descartes)

            es_tecnologica.append(contiene_tec and not contiene_no_tec)
            es_no_tecnologica.append(contiene_no_tec)

        self.df["es_tecnologica"] = es_tecnologica
        self.df["es_no_tecnologica"] = es_no_tecnologica

        print("‚úÖ Clasificaci√≥n tecnologica/no teconol√≥gica completada.")
        return self.df


    def procesar_completo(self):
        """
        Aplica todo el flujo: extracci√≥n de texto, limpieza, LDA y clasificaci√≥n manual.
        Elimina la variable temporal 'textos_limpios' al final.
        """
        print("üöÄ Iniciando procesamiento completo...")

        # 1. Procesar textos (extrae, limpia, aplica LDA)
        self._procesar_textos()

        # 2. Aplicar clasificaci√≥n tecnol√≥gica
        self.aplicar_clasificacion_manual()

        # 3. Eliminar variable temporal si se desea no guardar tokens
        if hasattr(self, "textos_limpios"):
            del self.textos_limpios
            print("üóëÔ∏è Variable 'textos_limpios' eliminada tras el procesamiento.")

        print("‚úÖ Procesamiento completo finalizado.")
        return self.df


